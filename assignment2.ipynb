{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Реализуйте полносвязную нейронную сеть с помощью библиотеки Tensor Flow. В качестве алгоритма оптимизации можно использовать, например, стохастический градиент (Stochastic Gradient Descent, SGD). Определите количество скрытых слоев от 1 до 5, количество нейронов в каждом из слоев до нескольких сотен, а также их функции активации (кусочно-линейная, сигмоидная, гиперболический тангенс и т.д.)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "\n",
    "    del save  # for gc\n",
    "\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0] * 100.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_steps = 5000\n",
    "hidden_size1 = 300\n",
    "hidden_size2 = 300\n",
    "hidden_size3 = 300\n",
    "beta_val = np.logspace(-4, -2, 20)\n",
    "accuracy_val = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-6278ed118802>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables\n",
    "    W1 = tf.get_variable('W1', [image_size * image_size, hidden_size1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [hidden_size1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 1\n",
    "    W2 = tf.get_variable('W2', [hidden_size1, hidden_size2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [hidden_size2], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 2\n",
    "    W3 = tf.get_variable('W3', [hidden_size2, hidden_size3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', [hidden_size3], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 3\n",
    "    W4 = tf.get_variable('W4', [hidden_size3, num_labels], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.get_variable('b4', [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Training computation\n",
    "    a1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "    a3 = tf.nn.relu(tf.matmul(a2, W3) + b3)\n",
    "    logits = tf.matmul(a3, W4) + b4\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    a1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    a2_valid = tf.nn.relu(tf.matmul(a1_valid, W2) + b2)\n",
    "    a3_valid = tf.nn.relu(tf.matmul(a2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(a3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    a1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    a2_test = tf.nn.relu(tf.matmul(a1_test, W2) + b2)\n",
    "    a3_test = tf.nn.relu(tf.matmul(a2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(a3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/konstantin/PycharmProjects/deep-learning-course/env/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Minibatch loss at step 0: 2.334985\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 27.5%\n",
      "Minibatch loss at step 500: 0.458464\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1000: 0.464164\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 1500: 0.489116\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2000: 0.302112\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2500: 0.320131\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3000: 0.385310\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 3500: 0.326352\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.385051\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.199415\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 94.9%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание 2. Как улучшилась точность классификатора по сравнению с логистической регрессией?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Я использовал полносвязную нейронную сеть с тремя слоями по 300 нейронов в каждой,\n",
    "с функцией активации RelU на скрытых слоях и softmax на выходном слое.\n",
    "Точность классификатора выросла на 5.2 процента по сравнению с логистической регрессией:\n",
    "теперь она составляет 94.8%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание 3. Используйте регуляризацию и метод сброса нейронов (dropout) для борьбы с переобучением. Как улучшилось качество классификации?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    regularization = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables\n",
    "    W1 = tf.get_variable('W1', [image_size * image_size, hidden_size1],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [hidden_size1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 1\n",
    "    W2 = tf.get_variable('W2', [hidden_size1, hidden_size2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [hidden_size2], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 2\n",
    "    W3 = tf.get_variable('W3', [hidden_size2, hidden_size3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', [hidden_size3], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 3\n",
    "    W4 = tf.get_variable('W4', [hidden_size3, num_labels], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.get_variable('b4', [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Training computation\n",
    "    a1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "    a3 = tf.nn.relu(tf.matmul(a2, W3) + b3)\n",
    "    logits = tf.matmul(a3, W4) + b4\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "           regularization * (\n",
    "                   tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) +\n",
    "                   tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) +\n",
    "                   tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) +\n",
    "                   tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    a1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    a2_valid = tf.nn.relu(tf.matmul(a1_valid, W2) + b2)\n",
    "    a3_valid = tf.nn.relu(tf.matmul(a2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(a3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    a1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    a2_test = tf.nn.relu(tf.matmul(a1_test, W2) + b2)\n",
    "    a3_test = tf.nn.relu(tf.matmul(a2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(a3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "L2 regularization(beta=0.00010) Test accuracy: 94.2%\n",
      "L2 regularization(beta=0.00013) Test accuracy: 94.8%\n",
      "L2 regularization(beta=0.00016) Test accuracy: 94.5%\n",
      "L2 regularization(beta=0.00021) Test accuracy: 94.5%\n",
      "L2 regularization(beta=0.00026) Test accuracy: 94.6%\n",
      "L2 regularization(beta=0.00034) Test accuracy: 94.2%\n",
      "L2 regularization(beta=0.00043) Test accuracy: 94.5%\n",
      "L2 regularization(beta=0.00055) Test accuracy: 94.2%\n",
      "L2 regularization(beta=0.00070) Test accuracy: 94.0%\n",
      "L2 regularization(beta=0.00089) Test accuracy: 92.9%\n",
      "L2 regularization(beta=0.00113) Test accuracy: 93.7%\n",
      "L2 regularization(beta=0.00144) Test accuracy: 92.5%\n",
      "L2 regularization(beta=0.00183) Test accuracy: 93.1%\n",
      "L2 regularization(beta=0.00234) Test accuracy: 92.3%\n",
      "L2 regularization(beta=0.00298) Test accuracy: 91.9%\n",
      "L2 regularization(beta=0.00379) Test accuracy: 91.7%\n",
      "L2 regularization(beta=0.00483) Test accuracy: 91.4%\n",
      "L2 regularization(beta=0.00616) Test accuracy: 89.7%\n",
      "L2 regularization(beta=0.00785) Test accuracy: 87.9%\n",
      "L2 regularization(beta=0.01000) Test accuracy: 87.3%\n",
      "Best beta=0.000127, accuracy=94.8%\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for beta in beta_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, regularization: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        print(\"L2 regularization(beta=%.5f) Test accuracy: %.1f%%\" % (beta, accuracy(test_prediction.eval(), test_labels)))\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "print('Best beta=%f, accuracy=%.1f%%' % (beta_val[np.argmax(accuracy_val)], max(accuracy_val)))\n",
    "plt.semilogx(beta_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для данной НС регуляризация почти не улучшила точность классификатора (+0.1% в сравнении с предыдущей моделью).\n",
    "Далее будем использовать значении регуляризации 0.00016, при котором НС показала наилучшую точность 94.9%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-338d8966f1d4>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    regularization = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables\n",
    "    W1 = tf.get_variable('W1', [image_size * image_size, hidden_size1],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [hidden_size1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 1\n",
    "    W2 = tf.get_variable('W2', [hidden_size1, hidden_size2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [hidden_size2], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 2\n",
    "    W3 = tf.get_variable('W3', [hidden_size2, hidden_size3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', [hidden_size3], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hidden layer 3\n",
    "    W4 = tf.get_variable('W4', [hidden_size3, num_labels], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.get_variable('b4', [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Training computation\n",
    "    a1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    a1 = tf.nn.dropout(a1, 0.9)\n",
    "    a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "    a2 = tf.nn.dropout(a2, 0.9)\n",
    "    a3 = tf.nn.relu(tf.matmul(a2, W3) + b3)\n",
    "    a3 = tf.nn.dropout(a3, 0.9)\n",
    "    logits = tf.matmul(a3, W4) + b4\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "           regularization * (\n",
    "                   tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) +\n",
    "                   tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) +\n",
    "                   tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) +\n",
    "                   tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    a1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    a2_valid = tf.nn.relu(tf.matmul(a1_valid, W2) + b2)\n",
    "    a3_valid = tf.nn.relu(tf.matmul(a2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(a3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    a1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    a2_test = tf.nn.relu(tf.matmul(a1_test, W2) + b2)\n",
    "    a3_test = tf.nn.relu(tf.matmul(a2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(a3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Minibatch loss at step 0: 2.421176\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 35.6%\n",
      "Minibatch loss at step 500: 0.589561\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1000: 0.599794\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1500: 0.577289\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 2000: 0.446838\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2500: 0.496118\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.466086\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3500: 0.425851\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 0.456536\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4500: 0.358285\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Dropout Test accuracy: 94.7%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, regularization: 0.00016}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Dropout Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Применение техники dropout не привело к увеличению качества классфикации.\n",
    "Вероятно НС не имеет признаков переобучения и поэтому, в данном случае, применение dropout только увеличивает ошибку обучения."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание 4. Воспользуйтесь динамически изменяемой скоростью обучения (learning rate). Наилучшая точность, достигнутая с помощью данной модели составляет 97.1%. Какую точность демонстрирует Ваша реализованная модель?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    regularization = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_size1], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size1]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size1, hidden_size2], stddev=np.sqrt(2.0 / hidden_size1)))\n",
    "    b2 = tf.Variable(tf.zeros([hidden_size2]))\n",
    "\n",
    "    W3 = tf.Variable(tf.truncated_normal([hidden_size2, hidden_size3], stddev=np.sqrt(2.0 / hidden_size2)))\n",
    "    b3 = tf.Variable(tf.zeros([hidden_size3]))\n",
    "\n",
    "    W4 = tf.Variable(tf.truncated_normal([hidden_size3, num_labels], stddev=np.sqrt(2.0 / hidden_size3)))\n",
    "    b4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation\n",
    "    a1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "    a3 = tf.nn.relu(tf.matmul(a2, W3) + b3)\n",
    "    logits = tf.matmul(a3, W4) + b4\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "           regularization * (\n",
    "                   tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) +\n",
    "                   tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) +\n",
    "                   tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) +\n",
    "                   tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4))\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.7, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    a1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    a2_valid = tf.nn.relu(tf.matmul(a1_valid, W2) + b2)\n",
    "    a3_valid = tf.nn.relu(tf.matmul(a2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(a3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    a1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    a2_test = tf.nn.relu(tf.matmul(a1_test, W2) + b2)\n",
    "    a3_test = tf.nn.relu(tf.matmul(a2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(a3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "num_steps = 5000\n",
    "hidden_size1 = 4000\n",
    "hidden_size2 = 2000\n",
    "hidden_size3 = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Minibatch loss at step 0: 2.452325\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 33.5%\n",
      "Minibatch loss at step 500: 0.582550\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000: 0.533436\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 1500: 0.580822\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.385167\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 2500: 0.437295\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 3000: 0.421934\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 3500: 0.409220\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 4000: 0.382378\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 4500: 0.302190\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Final Test accuracy: 95.3%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, regularization: 0.00016}\n",
    "\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "С помощью динамически изменяемой скорости обучения удалось достичь точности 95.3%\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}